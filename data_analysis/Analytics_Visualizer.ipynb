{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc1669f-7e7a-4765-821f-a4708bd53190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import parse_bdd100k_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5281866a-56ce-4071-96a5-8d3ac7e9d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/data_analysis\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16aa62c-5c0f-46ac-9a22-a23ac9e9bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = \"/workspace/bdd100k_labels_release/bdd100k/labels/bdd100k_labels_images_train.json\"\n",
    "val_json = \"/workspace/bdd100k_labels_release/bdd100k/labels/bdd100k_labels_images_val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e130a-01cb-4785-afeb-592c61b37ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_bdd100k_labels(train_json)\n",
    "print(df.head())\n",
    "print(f\"\\nParsed {len(df)} annotations across {df['image_name'].nunique()} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91434501-5276-49a2-a92d-ecd15bacdd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import plot_bdd_piecharts\n",
    "plot_bdd_piecharts(train_json, val_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa0a5d-03b4-46ff-9eb8-d31cfbb94cf4",
   "metadata": {},
   "source": [
    "# Image Attribute Distribution\n",
    "\n",
    "### Overview\n",
    "1. Image attributes in the dataset include **weather**, **scene**, and **timeofday**.  \n",
    "2. The distribution between the **validation** and **training** sets is reasonably similar.  \n",
    "3. However, each attribute in the dataset shows an **overwhelming bias (>50%)** toward a single category.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "As seen from the dataset:\n",
    "\n",
    "- **Weather:** Most images are captured in **clear weather** conditions.  \n",
    "- **Scene:** Majority of images depict **city streets**.  \n",
    "- **Time of Day:** Most images were taken during **daytime**.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "### Recommended Data Augmentations\n",
    "\n",
    "1. **HSV Value (`hsv_v: 0.015`)**  \n",
    "   - Minor adjustment to maintain **color consistency** across different camera sensors and lighting conditions.  \n",
    "\n",
    "2. **HSV Saturation (`hsv_s: 0.7`)**  \n",
    "   - Adjusts color vividness â€” making images more **vibrant** or **washed out**.  \n",
    "   - Useful for handling **overcast vs. clear weather** conditions.  \n",
    "   - High variation is suggested to mitigate **weather distribution imbalance**.  \n",
    "\n",
    "3. **HSV Brightness (`hsv_v: 0.4`)**  \n",
    "   - Alters image brightness (darker or brighter).  \n",
    "   - Helps handle **day vs. night** distribution imbalance.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d63ab-3bf0-4d12-902c-40b6382ec206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import plot_class_distribution_pie\n",
    "train_counts, val_counts = plot_class_distribution_pie(train_json, val_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb36188-494b-474c-ae1f-8b5fe414f512",
   "metadata": {},
   "source": [
    "# Class Distribution Analysis\n",
    "\n",
    "### Overview\n",
    "1. The **class distribution** of the training and validation sets is **nearly identical**.  \n",
    "2. However, the distribution is **heavily skewed** toward the **car** category, which represents **55.4%** of the dataset.  \n",
    "3. The **train** category is **severely underrepresented**, accounting for less than **0.002%** of the data (only **136 annotations** in the training set).  \n",
    "4. A few **incorrect annotations** have been observed in the **train** category.  \n",
    "5. Some **annotation errors** have also been noted in the **rider** class.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "### Recommended Augmentation Strategies\n",
    "\n",
    "1. **Mosaic Augmentation**  \n",
    "   - Combines multiple images into one, effectively **upsampling rare classes** without explicit oversampling.  \n",
    "   - Helps balance class frequency while maintaining spatial context.\n",
    "\n",
    "2. **Varifocal Loss**  \n",
    "   - Encourages the model to place **greater emphasis on rare classes** during training.  \n",
    "   - Improves performance in **class-imbalanced datasets** by adjusting the focus dynamically based on confidence and sample rarity.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f6dab4-05e7-48f4-8d3d-a7a909a0f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import plot_annotations_per_image\n",
    "train_counts, val_counts, stats_df = plot_annotations_per_image(train_json, val_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41120504-9ed4-4010-8228-93fb2023d9d2",
   "metadata": {},
   "source": [
    "# Annotations per Image\n",
    "\n",
    "### Overview\n",
    "1. **Annotation density** is roughly consistent between the **training** and **validation** sets.  \n",
    "2. The dataset exhibits a **moderate annotation density** overall.  \n",
    "3. Some **crowded scenes** are present, with up to **91 objects per image**.  \n",
    "4. There are **no empty images** in the dataset.  \n",
    "5. **No sampling bias** observed â€” annotation consistency is maintained between the train and validation sets.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. **Anchor-Free Models**  \n",
    "   - Better suited for **handling overlapping objects** compared to anchor-based models.  \n",
    "   - Improve detection in **crowded or high-density scenes**.\n",
    "\n",
    "2. **FPN / PAN Architectures (as in YOLOv8)**  \n",
    "   - Utilize **Feature Pyramid Network (FPN)** and **Path Aggregation Network (PAN)** structures.  \n",
    "   - These allow the model to **combine local details** and **broader contextual information**, enhancing detection performance across different scales.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b87de-9b5e-43a1-b862-4c101f528e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import compare_mean_relative_bbox_size\n",
    "compare_df = compare_mean_relative_bbox_size(train_json, val_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce5e78-39da-4944-9c7a-e2aa74d8710c",
   "metadata": {},
   "source": [
    "# Mean Relative Size per Class\n",
    "\n",
    "### Overview\n",
    "1. There is a **huge variation in object scales** across classes.  \n",
    "2. A significant portion of annotations fall under the **tiny to small** category.  \n",
    "3. This indicates that the detector must effectively handle **multi-scale variations** during training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. **Multi-Stage Detection Head (YOLOv8 Architecture)**  \n",
    "   - Enables simultaneous detection of **small** and **large** objects.  \n",
    "   - Improves feature representation across multiple scales, enhancing detection accuracy for size-diverse classes.\n",
    "\n",
    "2. **Higher Input Resolution**  \n",
    "   - Helps **preserve features** of small and tiny objects that might otherwise be lost at lower resolutions.  \n",
    "   - Beneficial when dealing with datasets dominated by **fine-grained, small-scale objects**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d024f39-371a-4ec6-b5f0-dd33e6e31544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from analyze import plot_aspect_ratio_distribution\n",
    "train_summary, val_summary = plot_aspect_ratio_distribution(train_json, val_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae2a1e-f005-41df-a5f9-287b24d18fe0",
   "metadata": {},
   "source": [
    "# Aspect Ratio per Class\n",
    "\n",
    "### Overview\n",
    "1. **Aspect ratio diversity** is high, with mean values ranging from **0.46 to 3.73**.  \n",
    "2. There is also **significant variation within individual classes**.  \n",
    "3. The **train** and **traffic sign** classes exhibit the **greatest variability** in aspect ratios.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. **Avoid Rotation Augmentation**  \n",
    "   - Many classes display **consistent aspect ratio distributions**, so avoiding rotation helps the model **retain shape context** and improves generalization for shape-based recognition.  \n",
    "\n",
    "2. **Anchor-Free Models (e.g., YOLOv8)**  \n",
    "   - Better equipped to handle **high aspect ratio diversity** compared to anchor-based approaches.  \n",
    "   - Improves bounding box adaptability across objects with irregular proportions.  \n",
    "\n",
    "3. **Varifocal + DFL Loss**  \n",
    "   - Enhances YOLOv8â€™s tolerance to **variable box shapes** and improves training stability.  \n",
    "   - Enables the model to assign **dynamic confidence weighting** based on bounding box quality and aspect ratio variation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e6b7e-dfb0-401d-90a7-8384a9eea2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import compute_mean_iou_matrix, plot_iou_heatmap\n",
    "iou_matrix, overlap_counts = compute_mean_iou_matrix(df, iou_thresh=0.5)\n",
    "\n",
    "print(\"\\nðŸ“Š Mean IoU Overlaps per Class Pair (IoU â‰¥ 0.5):\")\n",
    "print(iou_matrix.round(3))\n",
    "\n",
    "plot_iou_heatmap(iou_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278a139-036e-4a54-b4f2-cbed828af3b6",
   "metadata": {},
   "source": [
    "# Mean IoU Overlap per Class Pair\n",
    "\n",
    "### Overview\n",
    "1. The dataset shows **moderate overlap** between certain object classes.  \n",
    "2. This can cause **true positives** to be mistakenly suppressed by standard **Non-Maximum Suppression (NMS)**.  \n",
    "3. Proper handling of overlapping detections is crucial to preserve detection accuracy, especially in crowded scenes.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. **Soft-NMS**  \n",
    "   - Use **Soft Non-Maximum Suppression** to prevent moderate overlaps between true positives from being wrongly discarded.  \n",
    "   - Instead of completely removing overlapping boxes, Soft-NMS **reduces their confidence scores** based on IoU, allowing the detector to retain valid overlapping detections.\n",
    "\n",
    "2. **IoU-Aware Loss Functions (e.g., Varifocal Loss)**  \n",
    "   - Implementing **IoU-aware loss** during training helps the model better understand spatial overlap between bounding boxes.  \n",
    "   - **Varifocal Loss** improves confidence calibration by aligning predicted scores with IoU values, enhancing performance in **densely overlapped** object regions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d0193-3f94-47b3-b632-1d569a1dfc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import compute_cooccurrence_matrix, summarize_cooccurrence, plot_cooccurrence_heatmap\n",
    "\n",
    "co_matrix = compute_cooccurrence_matrix(df)\n",
    "\n",
    "print(\"\\nðŸ“Š Inter-class Co-occurrence Matrix (Train Set):\")\n",
    "print(co_matrix.round(3))\n",
    "\n",
    "summarize_cooccurrence(co_matrix)\n",
    "plot_cooccurrence_heatmap(co_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
