# Training hyperparameters for YOLOv8m-P2 on BDD100K

# Optimizer settings
optimizer: AdamW          # optimizer (SGD, Adam, AdamW, RMSProp)
lr0: 0.001               # initial learning rate (lower for AdamW)
lrf: 0.01                # final learning rate (lr0 * lrf)
momentum: 0.937          # SGD momentum/Adam beta1
weight_decay: 0.0005     # optimizer weight decay
warmup_epochs: 3.0       # warmup epochs
warmup_momentum: 0.8     # warmup initial momentum
warmup_bias_lr: 0.1      # warmup initial bias lr

# Loss gains (YOLOv8 uses VariFocal Loss + DFL by default)
box: 7.5                 # box loss gain
cls: 1.0                  # classification loss gain  
dfl: 1.5                 # distribution focal loss gain